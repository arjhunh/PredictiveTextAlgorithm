allowParallel = TRUE,
verboseIter = TRUE))
mod_rf <- train(clase ~ .,
data = data_train,
method = 'rf')
View(data_train)
mod_rf <- train(classe ~ .,
data = data_train,
method = 'rf')
source('~/.active-rstudio-document')
pml_train <- read.csv(file = 'data_train.csv',
na.strings = c('NA','#DIV/0!',''))
pml_quiz <- read.csv(file = 'data_test.csv',
na.strings = c('NA','#DIV/0!',''))
pml_train <- read.csv(file = 'data_train.csv',
na.strings = c('NA','#DIV/0!',''))
library(caret, quietly=TRUE)
url_train <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
url_test <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(url = url_train, destfile = 'data_train.csv')
download.file(url = url_test, destfile = 'data_test.csv')
pml_train <- read.csv(file = 'data_train.csv',
na.strings = c('NA','#DIV/0!',''))
pml_quiz <- read.csv(file = 'data_test.csv',
na.strings = c('NA','#DIV/0!',''))
for(i in c(8:ncol(pml_train)-1)) {
pml_train[,i] = as.numeric(as.character(pml_train[,i]))
pml_quiz[,i] = as.numeric(as.character(pml_quiz[,i]))
}
feature_index <- colnames(pml_train)
feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train); dim(data_xval)
mod_rf <- train(classe ~ ., data = data_train, method = 'rf', trControl = trainControl(method = "cv"))
mod_rf <- train(classe ~ ., data = data_train, method = 'rf', trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE, verboseIter = TRUE))
mod_rf <- randomForest(classe ~ ., data = data_train)
pred_rf <- predict(mod_rf,data_xval)
cm_rf <- confusionMatrix(pred_rf,data_xval$classe)
cm_rf
final_col <- length(colnames(pml_quiz[]))
colnames(pml_quiz)[final_col] <- 'classe'
quiz_rf <- predict(mod_rf,pml_quiz[,feature_index])
quiz_rf
mod_rf$finalModel
mod_rf
hist(data_training$classe)
hist(data_train$classe)
>hist
?hist
count(data_train$classe)
table(data_train$classe)
pml_train <- read.csv(file = 'data_train.csv'))##,na.strings = c('NA','#DIV/0!',''))
pml_train <- read.csv(file = 'data_train.csv')##,na.strings = c('NA','#DIV/0!',''))
pml_test <- read.csv(file = 'data_test.csv')##,na.strings = c('NA','#DIV/0!',''))
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
dim(pml_train)
dim(pml_test)
for(i in c(8:ncol(pml_train)-1)) {
pml_train[,i] = as.numeric(as.character(pml_train[,i]))
pml_quiz[,i] = as.numeric(as.character(pml_quiz[,i]))
}
feature_index <- colnames(pml_train)
feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
feature_index <- feature_index[-c(1:7)]
dim(pml_train)
str(pml_train)
pml_train <- read.csv(file = 'data_train.csv')
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
str(pml_train)
dim(pml_train)
pml_train<-pml_train[,-c(1:7)]
dim(pml_train)
feature_index <- colnames(pml_train)
feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train); dim(data_xval)
pml_train <- read.csv(file = 'data_train.csv')
pml_test <- read.csv(file = 'data_test.csv')
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
pml_train<-pml_train[,-c(1:7)]
pml_test<-pml_test[,-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train); dim(data_xval)
pml_train <- read.csv(file = 'data_train.csv')
pml_test <- read.csv(file = 'data_test.csv')
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
pml_train<-pml_train[,-c(1:7)]
pml_test<-pml_test[,-c(1:7)]
feature_index <- colnames(pml_train)
feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train); dim(data_xval)
pml_train <- read.csv(file = 'data_train.csv')
pml_test <- read.csv(file = 'data_test.csv')
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
pml_train<-pml_train[,-c(1:7)]
pml_test<-pml_test[,-c(1:7)]
# for(i in c(8:ncol(pml_train)-1)) {
#   pml_train[,i] = as.numeric(as.character(pml_train[,i]))
#   pml_quiz[,i] = as.numeric(as.character(pml_quiz[,i]))
# }
# feature_index <- colnames(pml_train)
# feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
# feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,]
data_xval <- pml_train[-index_train,]
dim(data_train); dim(data_xval)
rf_mod
mod_rf
cm_rf
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,]
data_xval <- pml_train[-index_train,]
dim(data_train); dim(data_xval)
table(data_train$classe)
##mod_rf <- train(classe ~ ., data = data_train, method = 'rf', trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE, verboseIter = TRUE))
mod_rf <- randomForest(classe ~ ., data = data_train)
mod_rf
pred_rf <- predict(mod_rf,data_xval)
cm_rf <- confusionMatrix(pred_rf,data_xval$classe)
cm_rf
##final_col <- length(colnames(pml_quiz[]))
##colnames(pml_quiz)[final_col] <- 'classe'
quiz_rf <- predict(mod_rf,pml_quiz)
quiz_rf
table(quiz_rf,pml_quiz[,"Classe"])
table(quiz_rf,pml_quiz[,53])
matrix(quiz_rf,pml_quiz[,53])
confusionMatrix(quiz_rf,pml_quiz$classe)
confusionMatrix(quiz_rf,pml_quiz[,53])
##testing <- read.csv(file = 'data_test.csv')
library(caret)
library(caret)
url_train <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
url_test <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(url = url_train, destfile = 'data_train.csv')
download.file(url = url_test, destfile = 'data_test.csv')
training <- read.csv(file = 'data_train.csv')
testing <- read.csv(file = 'data_test.csv')
training[training == ""]<-NA
training<-training[,which(as.numeric(colSums(is.na(training)))==0)]
testing[testing == ""]<-NA
testing<-testing[,which(as.numeric(colSums(is.na(testing)))==0)]
training<-training[,-c(1:7)]
testing<-testing[,-c(1:7)]
set.seed(1300)
trainindex <- createDataPartition(y=training$classe, p=0.80, list=FALSE)
data_train <- training[trainindex,]
data_val <- training[-trainindex,]
dim(data_train); dim(data_val)
model <- randomForest(classe ~ ., data = data_train)
model
predict_cv <- predict(model,data_val)
con_mat_cv <- confusionMatrix(predict_cv,data_val$classe)
con_mat_cv
predict_test <- predict(model,testing)
predict_test
?randomForest
??randomForest
install.packages("randomForest")
?download.file
install.packages("shiny")
install.packages("devtools")
install.packages("shinyapps")
install_github('slidify','ramnathv')
library(devtools)
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
require(devtools)
install_github("slidify", "ramnathv", ref = "dev")
install_github("slidifyLibraries", "ramnathv", ref = "dev")
?lm
?colSums
?predict
?colSums
?dgamma
?show
showMethods(show)
getMethog(show)
getMethod(show)
getClass(show)
getMethod(dgamma)
showMethods(dgamma)
?mean
?colSums
?mean
?lm
?dgamma
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
runApp(display.mode='showcase')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
runApp(display.mode='showcase')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
?random
??random
?sample
n=50
x=1:5
sam=sample(x,n,replace=TRUE,prob=c(0.15,0.15,0.15,0.4,0.15))
samp
sam
table(sam)
sam=sample(x,n,replace=TRUE,prob=c(0.0.8,0.0.05,0.05,0.05,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.0.05,0.05,0.05,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05,0.05,0.05,0.05))
table(sam)
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05,0.05,0.05))
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "en_US")
length(dir(cname))
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
class(docs)
summary(docs)
docs <- Corpus(DirSource(cname))
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "en_US")
length(dir(cname))
dir(cname)
docs <- Corpus(DirSource(cname))
docs
class(docs)
summary(docs)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
??content_transformer
library(tm) # Framework for text mining.
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
t
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
toSpace <- function(x, pattern) gsub(pattern, " ", x)
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "nn|")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content_transformer(tolower)) ## convert to lower case
docs <- tm_map(docs, tolower) ## convert to lower case
docs <- tm_map(docs, removePunctuation) ## remove punctuation
save.image("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/data memory.RData")
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka)
##Loading corpus and files
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "twitter")
length(dir(cname))
dir(cname)
docs <- Corpus(DirSource(cname))
toSpace <- function(x, pattern) gsub(pattern, " ", x) ##Replace charw with space
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, tolower) ## convert to lower case
docs <- tm_map(docs, removePunctuation) ## remove punctuation
docs <- tm_map(docs, removeWords, stopwords("english"))  ## remove common stop words
docs <- tm_map(docs, removeNumbers)    ## remove numbers
docs <- tm_map(docs, stripWhitespace)  ## remove whitespace
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
library(tm) # Framework for text mining.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka) ## N Grams
library(wordcloud) ## wordcloud
library(ggplot2)
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed")
docs <- Corpus(DirSource(cname))
tdm1 = TermDocumentMatrix(docs)
tdms1 <- removeSparseTerms(tdm1, 0.1)
dim(tdm1)
dim(tdms1)
freq <- sort(rowSums(as.matrix(tdms1)), decreasing=TRUE)
head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
subset(wf, freq>5000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq), freq, min.freq=1000, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=500, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=100, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(1, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(10, 9), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 3), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
plot(tdms,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.5)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.5)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.9)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=500)[1:10],corThreshold=0.9)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=1000)[1:10],corThreshold=0.9)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm2 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
rm(freq)
rm(wf)
freq1 <- sort(rowSums(as.matrix(tdms1)), decreasing=TRUE)
head(freq1, 14)
wf1 <- data.frame(word=names(freq1), freq=freq1)
head(wf1)
subset(wf1, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq1), freq1, min.freq=10, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=1000)[1:10],corThreshold=0.9)
freq2 <- sort(rowSums(as.matrix(tdms2)), decreasing=TRUE)
tdms2 <- removeSparseTerms(tdm2, 0.1)
freq2 <- sort(rowSums(as.matrix(tdms2)), decreasing=TRUE)
head(freq2, 14)
wf2 <- data.frame(word=names(freq2), freq=freq2)
head(wf2)
subset(wf2, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>5000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>4000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=3000, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(5, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=1000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms2,terms=findFreqTerms(tdms2, lowfreq=1000)[1:10],corThreshold=0.9)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm3 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
tdms3 <- removeSparseTerms(tdm3, 0.1)
freq3 <- sort(rowSums(as.matrix(tdms3)), decreasing=TRUE)
head(freq3, 14)
wf3 <- data.frame(word=names(freq3), freq=freq3)
head(wf3)
subset(wf3, freq>4000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>3000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf1, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq3), freq3, min.freq=1000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq3), freq3, min.freq=200, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=400, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=200, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
wordcloud(names(freq3), freq3, min.freq=200, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=250, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=300)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=200)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=250)[1:10],corThreshold=0.9)
cname1 <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "Final", "en_US")
length(dir(cname1))
dir(cname1)
docs1 <- Corpus(DirSource(cname1))
docs1
class(docs1)
cat("Summary of the blogs.txt file: \n",summary(docs1[[1]]))
cat("Summary of the blogs.txt file: \n")
cat("\n Number of lines in the file: ", length(docs1[[1]]))
cat("\nNumber of characters in the file: ",sum(nchar(docs1[[1]])))
cat("\nNumber of characters in the file: ",length(unlist(strsplit(docs1[[1]]," "))))
char1=sum(nchar(docs1[[1]]))
char2=sum(nchar(docs1[[2]]))
char3=sum(nchar(docs1[[3]]))
word1=length(unlist(strsplit(docs1[[1]]," ")))
word2=length(unlist(strsplit(docs1[[2]]," ")))
word3=length(unlist(strsplit(docs1[[3]]," ")))
install.packages("qdaptools")
y
install.packages("qdapTools")
save.image("~/RPUB.RData")
library(tm) # Framework for text mining.
library(qdap)# Quantitative discourse analysis of transcripts.
install.packages("qdap")
library(qdap)# Quantitative discourse analysis of transcripts.
install.packages("xlsxjars")
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka) ## N Grams
library(wordcloud) ## wordcloud
library(ggplot2)
install.packages("knitr")
install.packages("markdown")
library(markdown)
load("~/RPUB.RData")
subset(wf1, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq1), freq1, min.freq=10, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=1000)[1:10],corThreshold=0.9)
subset(wf2, freq>3000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq2), freq2, min.freq=1000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms2,terms=findFreqTerms(tdms2, lowfreq=1000)[1:10],corThreshold=0.9)
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq3), freq3, min.freq=250, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=250)[1:10],corThreshold=0.9)
install.packages("png")
install.packages("grid")
library(png)
library(grid)
length(docs1[[1]]))
length(docs1[[1]])
length(docs1[[2]])
length(docs1[[3]])
?readPNG
s="E:\Dropbox\Courses\DATA SCIENCE TRACK\10. Capstone Project\RPub1"
s="E:\\Dropbox\\Courses\\DATA SCIENCE TRACK\10. Capstone Project\RPub1"
s="E:\\Dropbox\\Courses\\DATA SCIENCE TRACK\10. Capstone Project\\RPub1"
s
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/RPub1")
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/RPub1")
