docs <- tm_map(docs, tonothing, "‘")
docs <- tm_map(docs, tonothing, "“")
docs <- tm_map(docs, tonothing, "”")
toString <- function(x, from, to) gsub(from, to, x)     ## specific transformations
docs <- tm_map(docs, toString, ",", "")
docs <- tm_map(docs, toString, "\\.", "")
docs <- tm_map(docs, toString, "’", "'")
docs <- tm_map(docs, toString, "'", "'")
docs <- tm_map(docs, toString, "â€™", "'")
docs <- tm_map(docs, toString, "â€", "'")
docs <- tm_map(docs, toString, " ™", "")
docs <- tm_map(docs, toString, "™", "")
docs <- tm_map(docs, tolower) ## convert to lower case
##docs <- tm_map(docs, removePunctuation) ## remove punctuation
##docs <- tm_map(docs, removeWords, stopwords("english"))  ## remove common stop words
##docs <- tm_map(docs, removeNumbers)    ## remove numbers
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project")
con = file("profanity.txt", "rb")
profanity=readLines(con)
close(con)
docs <- tm_map(docs, removeWords, profanity) ## remove bad words
docs <- tm_map(docs, stripWhitespace)  ## remove whitespace
docs[[1]][1]
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset/Processed file")
lapply(docs[[1]], write, "blog.txt", append=TRUE)
lapply(docs[[2]], write, "news.txt", append=TRUE)
lapply(docs[[3]], write, "twitter.txt", append=TRUE)
test=i can't breathe
test="i can't breathe"
?grep
sub("[a-zA-Z]’[a-zA-Z]","|",test)
grep("[a-zA-Z]’[a-zA-Z]",test)
grep("[a-z]’[a-z]",test)
grep("breathe",test)
grep("[a-z]",test)
grep("[a-z] [a-z]",test)
test="i can\t breathe"
test="i can+t breathe"
grep("[a-z]+[a-z]",test)
sub("[a-z]+[a-z]","-"test)
sub("[a-z]+[a-z]","-",test)
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "sampled")
docs <- Corpus(DirSource(cname))
docs[[1]][1]
docs[[1]][10]
docs[[1]][25]
docs[[1]][25]
docs[[1]][27]
docs[[1]][54]
docs[[1]][67]
docs[[1]][72]
docs[[1]][89]
docs[[1]][93]
docs[[1]][125]
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "sampled")
docs <- Corpus(DirSource(cname))
toSpace <- function(x, pattern) gsub(pattern, " ", x) ##Replace char with space
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, ";")
docs <- tm_map(docs, toSpace, "\\(")
docs <- tm_map(docs, toSpace, "\\)")
docs <- tm_map(docs, toSpace, "\\[")
docs <- tm_map(docs, toSpace, "\\]")
docs <- tm_map(docs, toSpace, "\\{")
docs <- tm_map(docs, toSpace, "\\}")
docs <- tm_map(docs, toSpace, "-")
toString <- function(x, from, to) gsub(from, to, x)     ## specific transformations
docs <- tm_map(docs, toString, ",", "")
docs <- tm_map(docs, toString, "\\.", "")
docs <- tm_map(docs, toString, "’", "'")
docs <- tm_map(docs, toString, "'", "'")
docs <- tm_map(docs, toString, "â€™", "'")
docs <- tm_map(docs, toString, "â€˜", "'")
docs <- tm_map(docs, toString, "™", "")
tonothing <- function(x, pattern) gsub(pattern, "", x) ##Replace char with nothing
docs <- tm_map(docs, tonothing, "â€œ")
docs <- tm_map(docs, tonothing, "â€")
docs <- tm_map(docs, tonothing, "!")
docs <- tm_map(docs, tonothing, "?")
docs <- tm_map(docs, tonothing, " [0-9] ")
docs <- tm_map(docs, tonothing, "\\$")
docs <- tm_map(docs, tonothing, "¦")
docs <- tm_map(docs, tonothing, "’ ")
docs <- tm_map(docs, tonothing, "’\n")
docs <- tm_map(docs, tonothing, "‘")
docs <- tm_map(docs, tonothing, "“")
docs <- tm_map(docs, tonothing, "”")
docs <- tm_map(docs, tolower) ## convert to lower case
##docs <- tm_map(docs, removePunctuation) ## remove punctuation
##docs <- tm_map(docs, removeWords, stopwords("english"))  ## remove common stop words
##docs <- tm_map(docs, removeNumbers)    ## remove numbers
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project")
con = file("profanity.txt", "rb")
profanity=readLines(con)
close(con)
docs <- tm_map(docs, removeWords, profanity) ## remove bad words
docs <- tm_map(docs, stripWhitespace)  ## remove whitespace
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset/Processed file")
lapply(docs[[1]], write, "blog.txt", append=TRUE)
lapply(docs[[2]], write, "news.txt", append=TRUE)
lapply(docs[[3]], write, "twitter.txt", append=TRUE)
?NGramTokenizer
?Weka_control
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed file")
docs <- Corpus(DirSource(cname))
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed file")
dir(cname)
docs <- Corpus(DirSource(cname))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm)
tdms <- removeSparseTerms(tdm, 0.1)
ms <- as.matrix(tdms)
write.csv(ms, file="tdms-6.csv")
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "sampled")
length(dir(cname))
dir(cname)
docs <- Corpus(DirSource(cname))
toSpace <- function(x, pattern) gsub(pattern, " ", x) ##Replace char with space
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, ";")
docs <- tm_map(docs, toSpace, "\\(")
docs <- tm_map(docs, toSpace, "\\)")
docs <- tm_map(docs, toSpace, "\\[")
docs <- tm_map(docs, toSpace, "\\]")
docs <- tm_map(docs, toSpace, "\\{")
docs <- tm_map(docs, toSpace, "\\}")
docs <- tm_map(docs, toSpace, "-")
toString <- function(x, from, to) gsub(from, to, x)     ## specific transformations
docs <- tm_map(docs, toString, ",", "")
docs <- tm_map(docs, toString, "\\.", "")
docs <- tm_map(docs, toString, "’", "'")
docs <- tm_map(docs, toString, "â€™", "'")
docs <- tm_map(docs, toString, "â€˜", "'")
docs <- tm_map(docs, toString, "™", "")
tonothing <- function(x, pattern) gsub(pattern, "", x) ##Replace char with nothing
docs <- tm_map(docs, tonothing, "â€œ")
docs <- tm_map(docs, tonothing, "â€")
docs <- tm_map(docs, tonothing, "!")
docs <- tm_map(docs, tonothing, "?")
docs <- tm_map(docs, tonothing, " [0-9] ")
docs <- tm_map(docs, tonothing, "[0-9][0-9] [0-9][0-9]")
docs <- tm_map(docs, tonothing, "\\$")
docs <- tm_map(docs, tonothing, "¦")
docs <- tm_map(docs, tonothing, "’ ")
docs <- tm_map(docs, tonothing, "’\n")
docs <- tm_map(docs, tonothing, "‘")
docs <- tm_map(docs, tonothing, "“")
docs <- tm_map(docs, tonothing, "”")
docs <- tm_map(docs, tolower) ## convert to lower case
##docs <- tm_map(docs, removePunctuation) ## remove punctuation
##docs <- tm_map(docs, removeWords, stopwords("english"))  ## remove common stop words
##docs <- tm_map(docs, removeNumbers)    ## remove numbers
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project")
con = file("profanity.txt", "rb")
profanity=readLines(con)
close(con)
docs <- tm_map(docs, removeWords, profanity) ## remove bad words
docs <- tm_map(docs, stripWhitespace)  ## remove whitespace
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset/Processed file")
lapply(docs[[1]], write, "blog.txt", append=TRUE)
lapply(docs[[2]], write, "news.txt", append=TRUE)
lapply(docs[[3]], write, "twitter.txt", append=TRUE)
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed file")
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed file")
docs <- Corpus(DirSource(cname))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm)
tdms <- removeSparseTerms(tdm, 0.1)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project")
ms <- as.matrix(tdms)
write.csv(ms, file="tdms-7.csv")
save.image("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/tdm and tdms.RData")
freq <- rowSums(as.matrix(tdm))
length(freq)
ord <- order(freq)
freq[head(ord)]
freq[tail(ord)]
head(table(freq), 15)
tail(table(freq), 15)
freq <- rowSums(as.matrix(tdms))
freq
table(freq)
findFreqTerms(tdms, lowfreq=1000)
findFreqTerms(tdms, lowfreq=100)
findAssocs(tdms, "happy", corlimit=0.001)
plot(tdms,
terms=findFreqTerms(tdms, lowfreq=100)[1:10],
corThreshold=0.5)
freq <- sort(rowSums(as.matrix(tdms)), decreasing=TRUE)
head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
library(ggplot2)
subset(wf, freq>500) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf, freq>5000) %>%
ggplot(aes(word, freq)) +
theme(axis.text.x=element_text(angle=45, hjust=1))
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf, freq>5000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
library(wordcloud)
wordcloud(names(freq), freq, min.freq=500, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
tdmsingle = DocumentTermMatrix(docs)
findFreqTerms(tdmsingle, lowfreq=1000)
findFreqTerms(tdms, lowfreq=100)
findAssocs(tdmsingle, "happy", corlimit=0.001)
findAssocs(tdmsingle, "happy", corlimit=0.6)
findAssocs(tdmsingle, "happy", corlimit=0.95)
plot(tdmsingle,
terms=findFreqTerms(tdms, lowfreq=100)[1:10],
corThreshold=0.5)
## plotting word frequencies
2360100/30
78670/20
3933.5*2
2360100/20
118005/2
?write
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM/prob mat")
setwd("C:/Users/arjhu_000/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM/prob mat")
uni=read.csv("uniprob.csv",header=TRUE)
bi=read.csv("biprob.csv",header=TRUE)
tri=read.csv("triprob.csv",header=TRUE)
quad=read.csv("quadprob.csv",header=TRUE)
pent=read.csv("pentprob.csv",header=TRUE)
hex=read.csv("hexprob.csv",header=TRUE)
View(uni)
uni[,1]=as.character(uni[,1])
bi[,1]=as.character(bi[,1])
tri[,1]=as.character(tri[,1])
quad[,1]=as.character(quad[,1])
pent[,1]=as.character(pent[,1])
hex[,1]=as.character(hex[,1])
readcharacter <- function()
{
n <- readline(prompt="Enter text: ")
return(as.character(n))
}
processing = function(input)
{
input=tolower(input)
input=gsub("~","",input)
input=gsub("`","",input)
input=gsub("!","",input)
input=gsub("@","",input)
input=gsub("#","",input)
input=gsub("\\$","",input)
input=gsub("\\%","",input)
input=gsub("^","",input)
input=gsub("\\&","",input)
input=gsub("*","",input)
input=gsub("\\(","",input)
input=gsub("\\)","",input)
input=gsub("-","",input)
input=gsub("_","",input)
input=gsub("=","",input)
input=gsub("+","",input)
input=gsub("\\|","",input)
input=gsub("\\]","",input)
input=gsub("\\}","",input)
input=gsub("\\[","",input)
input=gsub("\\{","",input)
input=gsub("\\' ","",input)
input=gsub(" \\'","",input)
input=gsub('"',"",input)
input=gsub(";","",input)
input=gsub(":","",input)
input=gsub("?","",input)
input=gsub("/","",input)
input=gsub("\\.","",input)
input=gsub(">","",input)
input=gsub(",","",input)
input=gsub("<","",input)
input=gsub("^\\'+|\\'+$", "", input)
input=gsub('^\\"+|\\"+$', "", input)
}
runalgo = function(input)
{
## input=processing(input)
nwords=sapply(strsplit(input, "\\s+"), length)
if(nwords>5)
{
split <- strsplit(input, " ")[[1]]
nwordstemp=length(split)
start=nwordstemp-4
splitreq=split[start:nwordstemp]
input=paste(splitreq, collapse = ' ')
nwords=5
}
if(nwords==5)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,hex$X)
if(length(q)!=0)
{
sub=hex[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==4)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,pent$X)
if(length(q)!=0)
{
sub=pent[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
## print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==3)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,quad$X)
if(length(q)!=0)
{
sub=quad[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
##print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==2)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,tri$X)
if(length(q)!=0)
{
sub=tri[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
##print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==1)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,bi$X)
if(length(q)!=0)
{
sub=bi[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
##  print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
## input=gsub("^.*? ","",input)
##  nwords=sapply(strsplit(input, "\\s+"), length)
print("the")
}
}
}
input=readcharacter()
input=processing(input)
nwords=sapply(strsplit(input, "\\s+"), length)
if(nwords>5)
{
split <- strsplit(input, " ")[[1]]
nwordstemp=length(split)
start=nwordstemp-4
splitreq=split[start:nwordstemp]
input=paste(splitreq, collapse = ' ')
nwords=5
}
if(nwords==5)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,hex$X)
if(length(q)!=0)
{
sub=hex[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
runalgo(input)
if(nwords==4)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,pent$X)
if(length(q)!=0)
{
sub=pent[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
## print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
input=readcharacter()
nwords=sapply(strsplit(input, "\\s+"), length)
if(nwords>5)
{
split <- strsplit(input, " ")[[1]]
nwordstemp=length(split)
start=nwordstemp-4
splitreq=split[start:nwordstemp]
input=paste(splitreq, collapse = ' ')
nwords=5
}
if(nwords==5)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,hex$X)
if(length(q)!=0)
{
sub=hex[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==4)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,pent$X)
if(length(q)!=0)
{
sub=pent[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
## print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
if(nwords==3)
{
e=paste0(input," ([A-Za-z]|[0-9])")
q=grep(e,quad$X)
if(length(q)!=0)
{
sub=quad[q,]
sub=sub[with(sub, order(-sub[,2])),]
tempsub=unlist(strsplit(sub[1,1]," "))
print(tempsub[nwords+1])
##print(sub('^.* ([[:alnum:]]+)$', '\\1', sub[1:5,1]))
}
if(length(q)==0)
{
input=gsub("^.*? ","",input)
nwords=sapply(strsplit(input, "\\s+"), length)
}
}
View(sub)
print(tempsub[nwords+1])
