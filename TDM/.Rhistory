pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
pml_train<-pml_train[,-c(1:7)]
pml_test<-pml_test[,-c(1:7)]
feature_index <- colnames(pml_train)
feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,feature_index]
data_xval <- pml_train[-index_train,feature_index]
dim(data_train); dim(data_xval)
pml_train <- read.csv(file = 'data_train.csv')
pml_test <- read.csv(file = 'data_test.csv')
pml_train[pml_train == ""]<-NA
pml_train<-pml_train[,which(as.numeric(colSums(is.na(pml_train)))==0)]
pml_test[pml_test == ""]<-NA
pml_test<-pml_test[,which(as.numeric(colSums(is.na(pml_test)))==0)]
pml_train<-pml_train[,-c(1:7)]
pml_test<-pml_test[,-c(1:7)]
# for(i in c(8:ncol(pml_train)-1)) {
#   pml_train[,i] = as.numeric(as.character(pml_train[,i]))
#   pml_quiz[,i] = as.numeric(as.character(pml_quiz[,i]))
# }
# feature_index <- colnames(pml_train)
# feature_index <- colnames(pml_train[colSums(is.na(pml_train)) == 0])
# feature_index <- feature_index[-c(1:7)]
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,]
data_xval <- pml_train[-index_train,]
dim(data_train); dim(data_xval)
rf_mod
mod_rf
cm_rf
set.seed(1300)
index_train <- createDataPartition(y=pml_train$classe, p=0.80, list=FALSE)
data_train <- pml_train[index_train,]
data_xval <- pml_train[-index_train,]
dim(data_train); dim(data_xval)
table(data_train$classe)
##mod_rf <- train(classe ~ ., data = data_train, method = 'rf', trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE, verboseIter = TRUE))
mod_rf <- randomForest(classe ~ ., data = data_train)
mod_rf
pred_rf <- predict(mod_rf,data_xval)
cm_rf <- confusionMatrix(pred_rf,data_xval$classe)
cm_rf
##final_col <- length(colnames(pml_quiz[]))
##colnames(pml_quiz)[final_col] <- 'classe'
quiz_rf <- predict(mod_rf,pml_quiz)
quiz_rf
table(quiz_rf,pml_quiz[,"Classe"])
table(quiz_rf,pml_quiz[,53])
matrix(quiz_rf,pml_quiz[,53])
confusionMatrix(quiz_rf,pml_quiz$classe)
confusionMatrix(quiz_rf,pml_quiz[,53])
##testing <- read.csv(file = 'data_test.csv')
library(caret)
library(caret)
url_train <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
url_test <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(url = url_train, destfile = 'data_train.csv')
download.file(url = url_test, destfile = 'data_test.csv')
training <- read.csv(file = 'data_train.csv')
testing <- read.csv(file = 'data_test.csv')
training[training == ""]<-NA
training<-training[,which(as.numeric(colSums(is.na(training)))==0)]
testing[testing == ""]<-NA
testing<-testing[,which(as.numeric(colSums(is.na(testing)))==0)]
training<-training[,-c(1:7)]
testing<-testing[,-c(1:7)]
set.seed(1300)
trainindex <- createDataPartition(y=training$classe, p=0.80, list=FALSE)
data_train <- training[trainindex,]
data_val <- training[-trainindex,]
dim(data_train); dim(data_val)
model <- randomForest(classe ~ ., data = data_train)
model
predict_cv <- predict(model,data_val)
con_mat_cv <- confusionMatrix(predict_cv,data_val$classe)
con_mat_cv
predict_test <- predict(model,testing)
predict_test
?randomForest
??randomForest
install.packages("randomForest")
?download.file
install.packages("shiny")
install.packages("devtools")
install.packages("shinyapps")
install_github('slidify','ramnathv')
library(devtools)
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
require(devtools)
install_github("slidify", "ramnathv", ref = "dev")
install_github("slidifyLibraries", "ramnathv", ref = "dev")
?lm
?colSums
?predict
?colSums
?dgamma
?show
showMethods(show)
getMethog(show)
getMethod(show)
getClass(show)
getMethod(dgamma)
showMethods(dgamma)
?mean
?colSums
?mean
?lm
?dgamma
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
runApp(display.mode='showcase')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
runApp(display.mode='showcase')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
shiny::runApp('C:/Users/Arjhun/Desktop/DevelopingDataProducts')
?random
??random
?sample
n=50
x=1:5
sam=sample(x,n,replace=TRUE,prob=c(0.15,0.15,0.15,0.4,0.15))
samp
sam
table(sam)
sam=sample(x,n,replace=TRUE,prob=c(0.0.8,0.0.05,0.05,0.05,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.0.05,0.05,0.05,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05,0.05,0.05,0.05))
table(sam)
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05))
sam=sample(x,n,replace=TRUE,prob=c(0.8,0.05,0.05,0.05))
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "en_US")
length(dir(cname))
dir(cname)
library(tm)
docs <- Corpus(DirSource(cname))
class(docs)
summary(docs)
docs <- Corpus(DirSource(cname))
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "en_US")
length(dir(cname))
dir(cname)
docs <- Corpus(DirSource(cname))
docs
class(docs)
summary(docs)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
??content_transformer
library(tm) # Framework for text mining.
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
t
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
toSpace <- function(x, pattern) gsub(pattern, " ", x)
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "nn|")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content_transformer(tolower)) ## convert to lower case
docs <- tm_map(docs, tolower) ## convert to lower case
docs <- tm_map(docs, removePunctuation) ## remove punctuation
save.image("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/data memory.RData")
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka)
##Loading corpus and files
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "final", "twitter")
length(dir(cname))
dir(cname)
docs <- Corpus(DirSource(cname))
toSpace <- function(x, pattern) gsub(pattern, " ", x) ##Replace charw with space
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, tolower) ## convert to lower case
docs <- tm_map(docs, removePunctuation) ## remove punctuation
docs <- tm_map(docs, removeWords, stopwords("english"))  ## remove common stop words
docs <- tm_map(docs, removeNumbers)    ## remove numbers
docs <- tm_map(docs, stripWhitespace)  ## remove whitespace
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
library(tm) # Framework for text mining.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka) ## N Grams
library(wordcloud) ## wordcloud
library(ggplot2)
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed")
docs <- Corpus(DirSource(cname))
tdm1 = TermDocumentMatrix(docs)
tdms1 <- removeSparseTerms(tdm1, 0.1)
dim(tdm1)
dim(tdms1)
freq <- sort(rowSums(as.matrix(tdms1)), decreasing=TRUE)
head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
subset(wf, freq>5000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq), freq, min.freq=1000, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=500, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=100, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(1, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(10, 9), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 3), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
plot(tdms,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.5)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.5)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=100)[1:10],corThreshold=0.9)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=500)[1:10],corThreshold=0.9)
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=1000)[1:10],corThreshold=0.9)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm2 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
rm(freq)
rm(wf)
freq1 <- sort(rowSums(as.matrix(tdms1)), decreasing=TRUE)
head(freq1, 14)
wf1 <- data.frame(word=names(freq1), freq=freq1)
head(wf1)
subset(wf1, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq1), freq1, min.freq=10, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
plot(tdms1,terms=findFreqTerms(tdms1, lowfreq=1000)[1:10],corThreshold=0.9)
freq2 <- sort(rowSums(as.matrix(tdms2)), decreasing=TRUE)
tdms2 <- removeSparseTerms(tdm2, 0.1)
freq2 <- sort(rowSums(as.matrix(tdms2)), decreasing=TRUE)
head(freq2, 14)
wf2 <- data.frame(word=names(freq2), freq=freq2)
head(wf2)
subset(wf2, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>5000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>4000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=3000, scale=c(5, 1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(5, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=2000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq2), freq2, min.freq=1000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms2,terms=findFreqTerms(tdms2, lowfreq=1000)[1:10],corThreshold=0.9)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm3 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
tdms3 <- removeSparseTerms(tdm3, 0.1)
freq3 <- sort(rowSums(as.matrix(tdms3)), decreasing=TRUE)
head(freq3, 14)
wf3 <- data.frame(word=names(freq3), freq=freq3)
head(wf3)
subset(wf3, freq>4000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf2, freq>3000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf1, freq>1000) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq3), freq3, min.freq=1000, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
subset(wf3, freq>400) %>%
ggplot(aes(word, freq)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wordcloud(names(freq3), freq3, min.freq=200, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(3, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=400, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=200, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
wordcloud(names(freq3), freq3, min.freq=200, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=300, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq3), freq3, min.freq=250, scale=c(2, 0.5), colors=brewer.pal(6, "Dark2"))
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=1000)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=300)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=200)[1:10],corThreshold=0.9)
plot(tdms3,terms=findFreqTerms(tdms3, lowfreq=250)[1:10],corThreshold=0.9)
cname1 <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Dataset", "Final", "en_US")
length(dir(cname1))
dir(cname1)
docs1 <- Corpus(DirSource(cname1))
docs1
class(docs1)
cat("Summary of the blogs.txt file: \n",summary(docs1[[1]]))
cat("Summary of the blogs.txt file: \n")
cat("\n Number of lines in the file: ", length(docs1[[1]]))
cat("\nNumber of characters in the file: ",sum(nchar(docs1[[1]])))
cat("\nNumber of characters in the file: ",length(unlist(strsplit(docs1[[1]]," "))))
char1=sum(nchar(docs1[[1]]))
char2=sum(nchar(docs1[[2]]))
char3=sum(nchar(docs1[[3]]))
word1=length(unlist(strsplit(docs1[[1]]," ")))
word2=length(unlist(strsplit(docs1[[2]]," ")))
word3=length(unlist(strsplit(docs1[[3]]," ")))
install.packages("qdaptools")
y
install.packages("qdapTools")
save.image("~/RPUB.RData")
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Tweet")
library(tm)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/Tweet")
# Collect data
tweets.mandrill<-read.csv('Mandrill.csv',header=T)
tweets.other<-read.csv('Other.csv',header=T)
tweets.test<-read.csv('Test.csv',header=T)
# Add "App" to mandrill tweets, and "Other" to others.
tweets.mandrill["class"]<-rep("App",nrow(tweets.mandrill))
tweets.other["class"]<-rep("Other",nrow(tweets.mandrill))
head(tweet.mandrill)
tweet.mandrill
tweets.mandrill
View(tweets.mandrill)
View(tweets.other)
View(tweets.test)
replacePunctuation <- function(x)
{
x <- tolower(x)
x <- gsub("[.]+[ ]"," ",x)
x <- gsub("[:]+[ ]"," ",x)
x <- gsub("[?]"," ",x)
x <- gsub("[!]"," ",x)
x <- gsub("[;]"," ",x)
x <- gsub("[,]"," ",x)
x
}
tweets.mandrill$Tweet <- replacePunctuation(tweets.mandrill$Tweet)
tweets.other$Tweet <- replacePunctuation(tweets.other$Tweet)
tweets.test$Tweet <- replacePunctuation(tweets.test$Tweet)
tweets.mandrill.corpus <- Corpus(VectorSource(as.vector(tweets.mandrill$Tweet)))
tweets.other.corpus <- Corpus(VectorSource(as.vector(tweets.other$Tweet)))
tweets.test.corpus <- Corpus(VectorSource(as.vector(tweets.test$Tweet)))
tweets.mandrill.matrix <- t(TermDocumentMatrix(tweets.mandrill.corpus,control = list(wordLengths=c(4,Inf))));
tweets.other.matrix <- t(TermDocumentMatrix(tweets.other.corpus,control = list(wordLengths=c(4,Inf))));
tweets.test.matrix <- t(TermDocumentMatrix(tweets.test.corpus,control = list(wordLengths=c(4,Inf))));
probabilityMatrix <-function(docMatrix)
{
# Sum up the term frequencies
termSums<-cbind(colnames(as.matrix(docMatrix)),as.numeric(colSums(as.matrix(docMatrix))))
# Add one
termSums<-cbind(termSums,as.numeric(termSums[,2])+1)
# Calculate the probabilties
termSums<-cbind(termSums,(as.numeric(termSums[,3])/sum(as.numeric(termSums[,3]))))
# Calculate the natural log of the probabilities
termSums<-cbind(termSums,log(as.numeric(termSums[,4])))
# Add pretty names to the columns
colnames(termSums)<-c("term","count","additive","probability","lnProbability")
termSums
}
tweets.other.matrix
inspect(tweets.other.matrix)
tweets.other.corpus
tweets.other.corpus[[1]]
tweets.other.corpus[[1]][1]
tweets.other.corpus[[1]][2]
tweets.other.corpus[[2]]
tweets.other.corpus[[2]][2]
tweets.mandrill.pMatrix<-probabilityMatrix(tweets.mandrill.matrix)
tweets.other.pMatrix<-probabilityMatrix(tweets.other.matrix)
tweets.other.pMatrix
tweets.mandrill.pMatrix<-probabilityMatrix(tweets.mandrill.matrix)
tweets.other.pMatrix<-probabilityMatrix(tweets.other.matrix)
write.csv(file="mandrillProbMatrix.csv",tweets.mandrill.pMatrix)
write.csv(file="otherProbMatrix.csv",tweets.mandrill.pMatrix)
tweets.test.matrix
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap)# Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz) # Correlation plots.
library(RWeka)
cname <- file.path("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project", "Dataset", "Processed")
docs <- Corpus(DirSource(cname))
tdm <- TermDocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
dim(tdm)
?removeSpareseTerms
?removeSparseTerms
tdms <- removeSparseTerms(tdm, 0.5)
dim(tdms)
setwd("E:\Dropbox\Courses\DATA SCIENCE TRACK\10. Capstone Project\TDM")
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
ms <- as.matrix(tdms)
write.csv(ms, file="tdms-unigram.csv")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm2 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm2)
tdms2 <- removeSparseTerms(tdm, 0.5)
dim(tdms2)
tdms2 <- removeSparseTerms(tdm, 0.1)
dim(tdms2)
tdms2 <- removeSparseTerms(tdm, 0.8)
dim(tdms2)
tdms <- removeSparseTerms(tdm, 0.7)
dim(tdms)
tdms2 <- removeSparseTerms(tdm2, 0.1)
dim(tdms2)
tdms2 <- removeSparseTerms(tdm2, 0.5)
dim(tdms2)
dim(tdm)
tdms <- removeSparseTerms(tdm, 0.5)
dim(tdms)
tdms <- removeSparseTerms(tdm, 0.6)
dim(tdms)
tdms <- removeSparseTerms(tdm, 0.8)
dim(tdms)
tdms <- removeSparseTerms(tdm, 0.7)
dim(tdms)
tdms <- removeSparseTerms(tdm, 0.5)
dim(tdms)
tdms2 <- removeSparseTerms(tdm2, 0.5)
dim(tdms2)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
ms <- as.matrix(tdms)
write.csv(ms, file="tdms-bigram.csv")
ms <- as.matrix(tdms2)
ms <- as.matrix(tdms2)
write.csv(ms, file="tdms-bigram.csv")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm3 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm3)
tdms3 <- removeSparseTerms(tdm3, 0.5)
dim(tdms3)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
ms <- as.matrix(tdms3)
write.csv(ms, file="tdms-trigram.csv")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm4 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm4)
tdms4 <- removeSparseTerms(tdm4, 0.5)
dim(tdms4)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
ms <- as.matrix(tdms4)
write.csv(ms, file="tdms-quadgram.csv")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm5 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm5)
tdms5 <- removeSparseTerms(tdm5, 0.5)
dim(tdms5)
ms <- as.matrix(tdms5)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
write.csv(ms, file="tdms-pentgram.csv")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6, delimiters = " \\r\\n\\t.,;:\"()?!"))
tdm6 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
dim(tdm6)
tdms6 <- removeSparseTerms(tdm6, 0.5)
dim(tdms6)
setwd("E:/Dropbox/Courses/DATA SCIENCE TRACK/10. Capstone Project/TDM")
ms <- as.matrix(tdms6)
write.csv(ms, file="tdms-hexgram.csv")
